{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# CSCI 3202, Spring 2022\n",
    "# Final Project\n",
    "\n",
    "<br> \n",
    "\n",
    "### Your name: Felipe Lima\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For my final project, I was decided to implement a Bayesian spam filtering. The idea was to create a spam filter to detect unsolicited and unwanted emails using Naive Bayes Theorem to calculate whether the message is spam or not. The system calculates the probability of a certain message being spam based on words in the title and message, learning from messages that were identified as spam and messages that were identified as not being spam. The scope of the project is based on a pre-defined data set and applying the algorithm to it would separate unwanted messages and normal email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing** some useful packages and libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading dataset** containing a collection of sms messages with SPAM and HAM messages:\n",
    "\n",
    "Dataset source: https://archive-beta.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages:\n",
      "5572\n",
      "Count: \n",
      "       Msg\n",
      "Type      \n",
      "ham   4825\n",
      "spam   747\n",
      "------\n",
      "Percentage: \n",
      "ham     86.593683\n",
      "spam    13.406317\n",
      "Name: Type, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                                Msg\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_or_ham = pd.read_csv('spam_ham_data', sep='\\t',\n",
    "names=['Type', 'Msg'])\n",
    "print(\"Total messages:\")\n",
    "print(spam_or_ham.shape[0])\n",
    "print(\"Count: \")\n",
    "print(spam_or_ham.groupby('Type').count())\n",
    "print(\"------\")\n",
    "print(\"Percentage: \")\n",
    "print(spam_or_ham['Type'].value_counts(normalize=True) *100)\n",
    "spam_or_ham.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in the imported data set there are a total of 5572 messages from which 4825 (86.59%) are normal messages (ham) and 747 (13.41%) are spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this assignment, ponctuations and non alphanumeric characters would just get in the way of proper working of the code so the next step was to **clean** the data set and set all characters to lower case so the comparison of words work the right way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, i, don, t, think, he, goes, to, usf, he,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                                Msg\n",
       "0   ham  [go, until, jurong, point, crazy, available, o...\n",
       "1   ham                     [ok, lar, joking, wif, u, oni]\n",
       "2  spam  [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3   ham  [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4   ham  [nah, i, don, t, think, he, goes, to, usf, he,..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_or_ham['Msg'] = spam_or_ham['Msg'].str.replace('\\W+', ' ').str.replace('\\s+', ' ').str.strip() #replaces any non alphanummeric character with a space, replaces any extra spaces with a single space and strips the string fro newlines and straineous characters\n",
    "spam_or_ham['Msg'] = spam_or_ham['Msg'].str.lower() #transfors all charactes into lower case\n",
    "spam_or_ham['Msg'] = spam_or_ham['Msg'].str.split() #splits the string into comma separated items on a list\n",
    "spam_or_ham.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the column \"Msg\" from the data set consists of a list of comma separated strings which will be easier to capture individual words and count the occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I **split** the dataset into a training set and a testing set. The training set is used so code learns what words are usually contained in spam emails and which ones are contained in normal messages. The testing set is used to calculate how well the algorithm did. To do so, first **randomizing** the full set guarantees that the training set and the test set contains roughly the same percentage of ham and spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages:\n",
      "5572\n",
      "Count: \n",
      "       Msg\n",
      "Type      \n",
      "ham   4825\n",
      "spam   747\n",
      "Percentage: \n",
      "ham     86.593683\n",
      "spam    13.406317\n",
      "Name: Type, dtype: float64\n",
      "------\n",
      "Training set:\n",
      "Total messages:\n",
      "4458\n",
      "Count: \n",
      "       Msg\n",
      "Type      \n",
      "ham   3858\n",
      "spam   600\n",
      "Percentage: \n",
      "ham     86.54105\n",
      "spam    13.45895\n",
      "Name: Type, dtype: float64\n",
      "------\n",
      "Total messages:\n",
      "1114\n",
      "Test set:\n",
      "Count: \n",
      "      Msg\n",
      "Type     \n",
      "ham   969\n",
      "spam  145\n",
      "Percentage: \n",
      "ham     86.983842\n",
      "spam    13.016158\n",
      "Name: Type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_set = spam_or_ham.sample(frac=0.8,random_state=1).reset_index(drop=True) #ramdomizes data set and gets 80% of it\n",
    "test_set = spam_or_ham.drop(train_set.index).reset_index(drop=True) #rest of the data set\n",
    "train_set = train_set.reset_index(drop=True)  #resets index\n",
    "print(\"Total messages:\")\n",
    "print(spam_or_ham.shape[0])\n",
    "print(\"Count: \")\n",
    "print(spam_or_ham.groupby('Type').count())\n",
    "print(\"Percentage: \")\n",
    "print(spam_or_ham['Type'].value_counts(normalize=True) *100)\n",
    "print(\"------\")\n",
    "print(\"Training set:\")\n",
    "print(\"Total messages:\")\n",
    "print(train_set.shape[0])\n",
    "print(\"Count: \")\n",
    "print(train_set.groupby('Type').count())\n",
    "print(\"Percentage: \")\n",
    "print(train_set['Type'].value_counts(normalize=True) *100)\n",
    "print(\"------\")\n",
    "print(\"Total messages:\")\n",
    "print(test_set.shape[0])\n",
    "print(\"Test set:\")\n",
    "print(\"Count: \")\n",
    "print(test_set.groupby('Type').count())\n",
    "print(\"Percentage: \")\n",
    "print(test_set['Type'].value_counts(normalize=True) *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After spliting the original data set into training and test, we have around the same percentage of ham (\\~87%) and spam (\\~13%) in all of them. The original set containing all the messages, the training set containing approximately 80% of the orignal set and the test set containing approximately 20% of the original set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I **created a list** of all unique words in all of the messages from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_set['Msg'].sum() #creates a list of all the words in the set\n",
    "vocab = set(vocab) #removes duplicate words from the list and make the list a set\n",
    "vocab = list(vocab) #reverts vocab to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I **created a dictionary** of all the words and the number of times they appear on the training set and made it into a data set that is then concatenated with the orginal training data set so we have the number of times each unique word appear on each unique message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Msg</th>\n",
       "      <th>ummifying</th>\n",
       "      <th>2wu</th>\n",
       "      <th>guessed</th>\n",
       "      <th>thread</th>\n",
       "      <th>hospital</th>\n",
       "      <th>pushbutton</th>\n",
       "      <th>needle</th>\n",
       "      <th>message</th>\n",
       "      <th>...</th>\n",
       "      <th>intro</th>\n",
       "      <th>smoking</th>\n",
       "      <th>july</th>\n",
       "      <th>withdraw</th>\n",
       "      <th>shake</th>\n",
       "      <th>tor</th>\n",
       "      <th>spotty</th>\n",
       "      <th>needing</th>\n",
       "      <th>gt</th>\n",
       "      <th>mumhas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>ham</td>\n",
       "      <td>[sorry, i, ll, call, later, in, meeting, any, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>ham</td>\n",
       "      <td>[babe, i, fucking, love, you, too, you, know, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>spam</td>\n",
       "      <td>[u, ve, been, selected, to, stay, in, 1, of, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>ham</td>\n",
       "      <td>[hello, my, boytoy, geeee, i, miss, you, alrea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>ham</td>\n",
       "      <td>[wherre, s, my, boytoy]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4458 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type                                                Msg  ummifying  2wu  \\\n",
       "0      ham                  [yep, by, the, pretty, sculpture]          0    0   \n",
       "1      ham  [yes, princess, are, you, going, to, make, me,...          0    0   \n",
       "2      ham                    [welp, apparently, he, retired]          0    0   \n",
       "3      ham                                           [havent]          0    0   \n",
       "4      ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...          0    0   \n",
       "...    ...                                                ...        ...  ...   \n",
       "4453   ham  [sorry, i, ll, call, later, in, meeting, any, ...          0    0   \n",
       "4454   ham  [babe, i, fucking, love, you, too, you, know, ...          0    0   \n",
       "4455  spam  [u, ve, been, selected, to, stay, in, 1, of, 2...          0    0   \n",
       "4456   ham  [hello, my, boytoy, geeee, i, miss, you, alrea...          0    0   \n",
       "4457   ham                            [wherre, s, my, boytoy]          0    0   \n",
       "\n",
       "      guessed  thread  hospital  pushbutton  needle  message  ...  intro  \\\n",
       "0           0       0         0           0       0        0  ...      0   \n",
       "1           0       0         0           0       0        0  ...      0   \n",
       "2           0       0         0           0       0        0  ...      0   \n",
       "3           0       0         0           0       0        0  ...      0   \n",
       "4           0       0         0           0       0        0  ...      0   \n",
       "...       ...     ...       ...         ...     ...      ...  ...    ...   \n",
       "4453        0       0         0           0       0        0  ...      0   \n",
       "4454        0       0         0           0       0        0  ...      0   \n",
       "4455        0       0         0           0       0        0  ...      0   \n",
       "4456        0       0         0           0       0        0  ...      0   \n",
       "4457        0       0         0           0       0        0  ...      0   \n",
       "\n",
       "      smoking  july  withdraw  shake  tor  spotty  needing  gt  mumhas  \n",
       "0           0     0         0      0    0       0        0   0       0  \n",
       "1           0     0         0      0    0       0        0   0       0  \n",
       "2           0     0         0      0    0       0        0   0       0  \n",
       "3           0     0         0      0    0       0        0   0       0  \n",
       "4           0     0         0      0    0       0        0   0       0  \n",
       "...       ...   ...       ...    ...  ...     ...      ...  ..     ...  \n",
       "4453        0     0         0      0    0       0        0   1       0  \n",
       "4454        0     0         0      0    0       0        0   0       0  \n",
       "4455        0     0         0      0    0       0        0   0       0  \n",
       "4456        0     0         0      0    0       0        0   0       0  \n",
       "4457        0     0         0      0    0       0        0   0       0  \n",
       "\n",
       "[4458 rows x 7785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates a dictionary of key being a unique word from vocab and entries composed of a list of size equal to the lenght of the set\n",
    "word_msg ={}\n",
    "for word in vocab:\n",
    "    word_msg.update({word: [0] * len(train_set['Msg'])})\n",
    "for i in range(len(train_set['Msg'])): #loops over the messages on the train set\n",
    "    for word in train_set['Msg'][i]: #loops over each word on the list of each Msg on the training set\n",
    "        word_msg[word][i] += 1 #increments the count of the word \"word\" on each \"Msg\" the \"word\" appears\n",
    "\n",
    "words_df = pd.DataFrame(word_msg) #make it into a dataframe and concatenates with training set\n",
    "train_set = pd.concat([train_set, words_df], axis=1) #concatenates with training set\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table we can see there are 7783 unique words in the data set. Since a lot of them are very specific the majority of the words are listed as 0 appearences as they only appear on a selected number of messages and due to the number of columns and rows not all of the words and messages are listed on the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data set that is going to be used to train the code is organized and clean I will move on the **calculations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate whether a specific message is spam or ham, we want to compare probabilities of the message being spam/ham given the content (text) of the message. Meaning:\n",
    "\n",
    "$$P(\\text{spam/ham}|w_1, w_2, w_3 ... w_n)$$\n",
    "\n",
    "Because we are applying Naive Bayes Theorem to the equation and we calculated the probability of discrete, individual words and not the probabilities of something continuous these probabilities are also called likelihoods. So we have that:\n",
    "\n",
    "$$P(\\text{spam/ham}|w_1, w_2, w_3 ... w_n) \\propto P(\\text{spam/ham}) \\cdot P(w_1|\\text{spam/ham}) \\cdot P(w_2|\\text{spam/ham}) \\cdot P(w_3|\\text{spam/ham}) \\cdot ... \\cdot P(w_n|\\text{spam/ham})$$\n",
    "\n",
    "and therefore:\n",
    "\n",
    "$$P(\\text{spam/ham}|w_1, w_2, w_3 ... w_n) \\propto P(\\text{spam/ham}) \\cdot \\prod_{i = 1}^{n} P(w_i|\\text{spam/ham})$$\n",
    "\n",
    "Great source to understand the calculations: https://www.youtube.com/watch?v=O2L2Uv9pdDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the spirit of facilitating the understanding of the code and reducing the number of calculations I **stored the constant values** in variables. The following code contains:\n",
    "- A data frame of spam and ham messages separetly\n",
    "- Probability of a message being ham or spam\n",
    "- The number of words in spam and ham messages separetly\n",
    "- The length of the vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_msgs = train_set[train_set['Type'] == 'spam'] #df of all spam messages\n",
    "ham_msgs = train_set[train_set['Type'] == 'ham'] #df of all ham messages\n",
    "\n",
    "p_ham = train_set['Type'].value_counts(normalize=True)[0] #probability of a message being ham\n",
    "p_spam = train_set['Type'].value_counts(normalize=True)[1] #probability of a message being spam\n",
    "\n",
    "nw_spam = len(spam_msgs['Msg'].sum()) #number of words in all the spam messages\n",
    "nw_ham = len(ham_msgs['Msg'].sum()) #number of words in all the ham messages\n",
    "\n",
    "len_vocab = len(vocab) #number of words in the vocab list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that constant values were stored, I'll **create helper functions** to calculate $P(w|spam)$ and $P(w|ham)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 #Laplace smoothing\n",
    "def pw_spam(word):\n",
    "    return (spam_msgs[word].sum() + alpha)  / (nw_spam + alpha*len_vocab)\n",
    "    \n",
    "def pw_ham(word):\n",
    "    return (ham_msgs[word].sum() + alpha)  / (nw_ham + alpha*len_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the above probabilities the formula used was the following:\n",
    "    $$P(w|spam/ham) = \\frac{\\text{# of times a word \"w\" occurs in in spam/ham messages} + \\alpha}{\\text{# of words in spam/ham messages} + \\alpha*\\text{total number of words in vocab}}$$\n",
    "    \n",
    "This accounts for the probability of a word given it is a spam/ham message + Laplace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly the identify function below is reponsible for calculating the probability of a message being spam or ham and comparing both to then identify whether the message is spam or ham. The greater probability is the classification of the message.\n",
    "\n",
    "The function calculates the probability following the formula:\n",
    "\n",
    "$$P(\\text{spam/ham}|w_1, w_2, w_3 ... w_n) \\propto P(\\text{spam/ham}) \\cdot \\prod_{i = 1}^{n} P(w_i|\\text{spam/ham})$$\n",
    "\n",
    "**Note** - if a word passed to the identify function is not present in the trained data set the function will just ignore it. The hope is that the data set is large enough to account for most words in the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify(msg, flag):\n",
    "    msg = re.sub('\\W', ' ', msg).lower().split() #clean message\n",
    "    p_msg_spam = 1\n",
    "    p_msg_ham = 1\n",
    "    for word in msg: #for every word in the message passed as input\n",
    "        if word in train_set.columns: #if the word is in the data set\n",
    "            p_msg_spam *= pw_spam(word) #multiply the prior probability to P(word|spam)\n",
    "            p_msg_ham *= pw_ham(word) #multiply the prior probability to P(word|ham)\n",
    "    \n",
    "    p_msg_spam *= p_spam # multiplies by the probability of spam\n",
    "    p_msg_ham *= p_ham # multiplies by the probability of spam\n",
    "    if (flag == True):\n",
    "        print('Probability of spam:', p_msg_spam)\n",
    "        print('Probability of ham:', p_msg_ham)\n",
    "\n",
    "    #compare probabilities to determine if a message is spam or ham\n",
    "    if p_msg_ham > p_msg_spam:\n",
    "        return 'ham'\n",
    "    elif p_msg_ham < p_msg_spam:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'equal'\n",
    "    \n",
    "def classi(classification):\n",
    "    if (classification == 'ham'):\n",
    "        print('The inputed message is: Ham')\n",
    "    elif (classification == 'spam'):\n",
    "        print('The inputed message is: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, human needed to identify message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for a user input. In this case the email sent with this project proposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of spam: 3.6782119079708777e-303\n",
      "Probability of ham: 2.5944121027711605e-285\n",
      "The inputed message is: Ham\n"
     ]
    }
   ],
   "source": [
    "user_input = \"For my final project, I was thinking of doing Bayesian spam filtering. The idea is to create a spam filter to detect unsolicited and unwanted emails using a bayesian system to calculate whether the message is spam or not. The system would calculate a probability of whether a certain message is spam based on words in the title and message, learning from messages that were identified as spam and messages that were identified as not being spam. The scope of the project would have to be based on a pre-defined data set and applying the algorithm to it would separate unwanted messages and normal email.Hope to hear from you soon whether this project is approved or not!\"\n",
    "result = identify(user_input, True)\n",
    "classi(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test with a larger number of inputs to test for accuracy of the algorithm. Using the test data set with 20% of the origial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classification: 1104\n",
      "Incorrect classification: 10\n",
      "Accuracy: 99.10233393177738 %\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for msg_i in range(len(test_set)): \n",
    "    if test_set['Type'][msg_i] == identify(str(test_set['Msg'][msg_i]), False):\n",
    "        counter += 1\n",
    "    \n",
    "print('Correct classification:', counter)\n",
    "print('Incorrect classification:', len(test_set) - counter)\n",
    "print('Accuracy:', counter/len(test_set)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on performance**\n",
    "\n",
    "As seen aboeve the filter had an accuracy of 99.1%, which is a promising result.\n",
    "\n",
    "Here you can test your own input. Just run the next cell and input your message!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try your own input**\n",
    "\n",
    "Just run the cell bellow and see if your input is classified as ham or spam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try your own input here!\n",
      "Probability of spam: 3.585454711237706e-15\n",
      "Probability of ham: 5.847714655437426e-13\n",
      "The inputed message is: Ham\n"
     ]
    }
   ],
   "source": [
    "input_message = input()\n",
    "classi(identify(input_message, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Notes on credit**\n",
    " \n",
    "In order to create this spam filter I based my project on different websites implementing a similar technique but typing explanations and code on my own. I did my best to optmize the code and I managed to get better results than the ones from websites I got my inspiration from. Here is a list of websites used:\n",
    "\n",
    "- https://archive-beta.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "- https://www.kdnuggets.com/2020/07/spam-filter-python-naive-bayes-scratch.html\n",
    "- https://www.youtube.com/watch?v=O2L2Uv9pdDA\n",
    "- https://towardsdatascience.com/logic-and-implementation-of-a-spam-filter-machine-learning-algorithm-a508fb9547bd\n",
    "- https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece#:~:text=Laplace%20smoothing%20is%20a%20smoothing%20technique%20that%20helps%20tackle%20the,the%20positive%20and%20negative%20reviews\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
